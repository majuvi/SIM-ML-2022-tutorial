---
title: "SMAP - tutorial for beginners"
output:
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data set

## Load the data set

We use a simulated data set of 711599 persons who are 18+ years old and representative of Amsterdam. The data frame contains the neighbourhood (bu_code), demographic features (age, sex, ...), and survey answers to the health monitor question "Drank Alcohol in the past 12 months" (y = 0,1) for every individual. This simulated data set also contains the true probability of drinking alcohol (p) for every person:

```{r}
library(readr)
library(dplyr)
library(tidyr)

# Load data set 
data <- read_csv("SMAP_simulated.zip", show_col_types=F) %>% 
  mutate_if(is.character, as.factor)
data
```


## Plotting the prevalences based on survey responses

> Small Area Estimation (SAE) seeks to predict the outcome in areas with only few people.

Our goal is to estimate how many people drink alcohol (y) in each area (bu_code) based on the survey. Calculate the true prevalence and population size in each area:

```{r}
# Calculate prevalence in each bu_code
bu_code.prevalence <- data %>% group_by(bu_code) %>% summarize(y=mean(y), n=n())
bu_code.prevalence
```

In reality, the problem is that we can have very few people answering the survey in an area, which means that taking the mean of their responses is very unreliable. But what if we could predict the missing survey responses for the entire population? Then we would have much more data in every area and make estimates like this.... 

Lets plot this data. In Small Area Estimation we try to make more accurate estimates of this map:

```{r}
library(sf)
library(stringr)
library(ggplot2)

# Fetch the geometry of the Netherlands from the internet
laad_geodata_wfs <- function(region="buurt", year=2020) {
  fn.cache <- sprintf("%s_%d.rds", region, year)
  # Read a local copy if fetched before
  if (file.exists(fn.cache)) {
    geo_data <- readRDS(fn.cache)
  } else {
    print("Info: Possible regions are gemeente, ggdregio, buurt, wijk)")
    # Set WFS web address. You can look this up in QGIS through the PDOK plugin
    # Or http://www.nationaalgeoregister.nl/geonetwork/srv/dut/catalog.search#/metadata/effe1ab0-073d-437c-af13-df5c5e07d6cd
    baseurl <- "https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?"
    wfsrequest <- sprintf("request=GetFeature&outputFormat=json&typeName=cbs_%s_%s_gegeneraliseerd", region, year)
    geo_data <- subset(st_read(str_c(baseurl, wfsrequest)), select=c(statcode, statnaam))
    geo_data$statcode <- as.factor(geo_data$statcode)
    saveRDS(geo_data, fn.cache)
  }
  return(geo_data)
}

# Load an sf geometry polygon (geometry) for each neighbourhood (bu_code)
bu_code.geometry <- laad_geodata_wfs(region="buurt", year=2020)
# Add geometry information and calculate quantiles
bu_code.geometry <- bu_code.geometry %>%
  rename(bu_code=statcode, name=statnaam) %>% 
  merge(bu_code.prevalence) %>% 
  mutate(prevalence=cut(y, quantile(y, probs=seq(0,1,0.1))))
# Plot
p1 <- ggplot(bu_code.geometry) + 
  geom_sf(aes(geometry=geometry, fill=prevalence), color=NA) +
  scale_fill_brewer(palette="RdYlGn", direction=-1) +
  labs(title="Small Area Estimates") + 
  theme(legend.position="bottom")
p1

```


# Evaluate the model with train & test for Amsterdam data

We now investigate the performance if we had the entire population of Amsterdam to train with.
You can think of this like "Ask people in Amsterdam, then generalize to the entire Netherlands".
This doesn't seem not too far from the scenario where around 4% of the population of Netherlands
had answered the survey and we predicted the prevalence in all neighbourhoods of Netherlands.

Advanced knowledge: is this an accurate estimate of generalization ability?

> How do we evaluate how well the model predicts?

One option is to split the observed survey responses data set into a "training" and "test" set. We know the true answers in the "test" set. The model is fitted to the "training" set and the predicted answers in the "test" set are compared to the true answers. It is essential that we compare the model predictions to outcomes that the model has not seen! Many machine learning methods are so flexible that they could fit the training data perfectly, but this might not generalize well outside that training set.

Split Amsterdam data into train & test (75% in train)

```{r}
set.seed(42)
train.idx <- sample.int(n=nrow(data), size=round(0.75*nrow(data)), replace=F)
train <-  (1:nrow(data) %in% train.idx)
test  <- !(1:nrow(data) %in% train.idx)

# How many rows?
print(paste(sum(train), sum(test), sep=" / "))
```

We use the AUC to compare the models in this example. Because we know the probabilities used to generate the data, we could define a perfect model

```{r warning=FALSE}
library(pROC)
calculate_auc <- function (y_true, y_pred) auc(roc(y_true, y_pred, levels=c(0, 1), direction = '<', quiet=T))

# These are the true answers we seek to predict
true_values <- data[test,]$y
true_probability <- data[test,]$p

# This is the best possible performance, with real data there is no way to know this
print(calculate_auc(true_values, true_probability)) 

```

Lets start with the SMAP model. This is the 'true model' we used to simulate the data. To train the model we must give an explicit model specification:

```{r warning=FALSE}
source("statistical_model.R")
# Train the STAR model
ggd.bu_codes <- list(GM0363=unique(data$bu_code))
model <- smapmodel(ggd.bu_codes, bu_code.geometry)
formula = y ~
  s(age, by = sex,  bs = "ps", k = 10) +
  s(age, by = ethnicity,  bs = "ps", k = 10) +
  s(age, by = marital_status, bs = "ps", k = 10) +
  s(age, by = education, bs = "ps", k = 10) +
  s(sex, ethnicity,  bs = "re") +
  s(sex, marital_status, bs = "re") +
  s(sex, education, bs = "re") +
  s(hhtype, bs = "re") +
  s(hhsize, bs = "ps", k = 5) +
  s(hhincomesource, bs = "re") +
  s(hhhomeownership, bs = "re") +
  s(hhincome, bs = "ps", k = 10) +
  s(hhassets, bs = "ps", k = 10) +
  s(oad, bs = "ps", k = 10)
model <- fit.smapmodel(model, data[train,], formula)
# Predict with the STAR model
predictions <- predict.smapmodel(model, data[test,])
# The true model does very well
print(calculate_auc(true_values, predictions))
```

It is very easy to train the XGBoost model and predict with it:

```{r warning=FALSE}
library(Matrix)
library(xgboost)

# Define a matrix of features as input and a vector of labels as output:
formula <- y ~ age + sex + ethnicity + marital_status + education +
  hhtype + hhsize + hhhomeownership + hhincomesource + hhincome + hhassets +
  oad + X + Y
X <- sparse.model.matrix(formula, data=data)
y <- data$y
# Train
model <- xgboost(objective="binary:logistic", eval_metric="logloss", data=X[train,], label=y[train], nrounds=50, verbose=0)
# Predict
predictions <- predict(model, X[test,])
# XGBoost is pretty good
print(calculate_auc(true_values, predictions))
```

Fit a simple linear model and predict the unknown outcomes

```{r}
# Train
model <- glm(formula, data = data[train,], family = "binomial")
# Predict
predictions <- predict(model, newdata = data[test,], type = "response")
# Simple logistic regression works quite well..
print(calculate_auc(true_values, predictions)) 
```

> What do you think, are these differences significant?


# Overfitting and adjusting hyperparameters

Why did we set the number of trees to 50? A lucky guess ;). But lets investigate more formally.

We use 5-fold cross validation on the training set and for the number of trees 1, 2, 3, ... , 300 we store:

1. Training error (mean AUC in training folds)
2. Validation error (mean AUC in validation folds)

> With XGBoost, training the model means adding one more tree at each iteration. 

Luckily XGBoost has a ready interface for this. We could also program it manually. This is the most important 'hyperparameter' of XGBoost, at least in most problems. Save the train & test AUC as a function of number of trees (nrounds):

```{r}
set.seed(42)
model <- xgb.cv(list(objective="binary:logistic", eval_metric="auc"), data=X[train,], label=y[train],
                nrounds=300, verbose=1, nfold=5, stratified=T, print_every_n=10)
```

Overfitting seems to occur after about 50 trees. If we fit more trees, training performance keeps improving but the test performance decreases.

```{r}
# Calculate max train and test AUC at different number of trees
evaluation.max <- model$evaluation %>% 
  gather(variable, value, c(train_auc_mean, test_auc_mean)) %>% 
  group_by(iter, variable) %>% 
  summarize(value=max(value))
# Plot the results
ggplot(evaluation.max, aes(x=iter, y=value, group=variable, color=variable)) + geom_line() +
  labs(title="XGBoost hyperparameters: number of trees", x="Iteration (nrounds)", y="AUC")
```

What is the optimal number of trees?

```{r}
opt.params <- evaluation.max %>% ungroup %>% filter(variable=="test_auc_mean") %>% top_n(n=1)
opt.params
```


> How much should you tune the hyperparameters for optimal results?


# Interpreting the model

## Plot learned trees 

Lets plot the first learned tree. What do we learn?

```{r}
library(DiagrammeR)
model <- xgboost(objective="binary:logistic", eval_metric="logloss", data=as.matrix(X[train,]), label=y[train], nrounds=opt.params$iter, verbose=0)
xgb.plot.tree(model=model, trees=0:0, width=2000, height=8000)
```

## Plot some SHAP values

SHAP values are one way to interpret the results. Every individual's predicted outcome
is decomposed into contribution of their features x_i1 + x_i2 + ... + x_id = y_i, where the
values of x_ij each get a Shapley value indicating their total contribution to the outcome.
The math is elegant but a bit complex... 

```{r}
library(DALEX)
library(DALEXtra)
# DALEX wraps different models inside a interface using an explainer object
explain_xgb <- explain_xgboost(model = model, data=as.matrix(X[train,]), y = y[train], label = "XGBoost")
```

Variable importance plot displays on the aggregate data set level which features are important for prediction:

```{r}
var_importance <- model_parts(explain_xgb)
plot(var_importance, show_boxplots=F)

```

On the aggregate data set level, we can use Partial Dependence Plots (PDPs) to investigate how a feature affects the predictions, for example:

```{r}
pdp_fare <- model_profile(explain_xgb, variables = "age", type = "partial")
plot(pdp_fare)
```


How to interpret predictions for a given person? The following person seems to have a very high probability of drinking alcohol:

```{r}
person <- as.matrix(X[2,,drop=F])
predict(explain_xgb, newdata = person)
```

The person is a 33 year old Dutch man who is single well-to-do entrepreneur with scientific education, Shapley values indicate these are all quite positive contributions to drinking:

```{r}
shap_xgb = predict_parts(explainer = explain_xgb, new_observation = person, type = "shap", N=10)
plot(shap_xgb, show_boxplots=F)
```


## Plot SMAP model terms

In this particular problem, we know the true 'white box' statistical model that defines the full probability distribution for the data. We can plot the terms of this statistical model to have an understandable interpretation:

```{r fig.width=8,fig.height=10,warning = FALSE,message = FALSE}
plot.smap(bu_code.geometry, "drinker_interpretation_smap.txt", ncol=3)
```

> Does this look like what the ML intepretation was telling us?


# Lessons from the tutorial

Start with a simple model. Machine learning can help if you have a complex target and lots of data.

* Target complexity: simple models are sufficient for simple relationships
* Amount of data: more data is needed to learn complex relationships

Do a train/test set split or cross-validation to test how well your model performs

* Assumption: both train and test set are sampled i.i.d. from the same source.
* Example violation: training on Amsterdam and testing on Netherlands!

Hyperparameter searches:

* It is essential not to overfit, use a validation set to tune the models with training data.
* Number of trees is an important hyperparameter for XGboost, you can use early stopping.

Interpreting the models:

* It is possible to plot the learned trees to understand the model. This can be complex to interpret.
* SHAP values, for example, can explain the predictions. These tell the same story as the true model.

> Conclusion: Use machine learning when the goal is to predict. It is often easier and more accurate than statistical models. But start with the simplest model.


# EXERCISES


**Exercise #1** : Try different machine learning models. For example, random forest. Does it do better?

```{r class.source = 'fold-hide', results='hide'}

```

**Exercise #2** : Try optimizing more hyperparameters. For example, the 'depth of trees' can also be important.  You can loop over different tree depths (max_depth, 6 is the default) and number of trees (niter) . You can save some time with 'early stopping'. As before, we save the validation AUC while we are training the model. When the validation error stops improving, we stop the training and return the optimal number of trees. This speeds up the hyperparameter search a bit. It can be done like this:

```{r}
# Train a model with early stopping
model <- xgb.cv(list(objective="binary:logistic", eval_metric="auc"), data=X[train,], label=y[train], 
                nrounds=300, verbose=1, nfold=5, stratified=T, print_every_n=10,
                early_stopping_rounds=10, max_depth=6)
```

Now optimize both depth of trees & number of trees.

```{r  class.source = 'fold-hide', results='hide'}

```

**EXERCISE #3**: In the original survey we had the answers for ~4% of the relevant population. What if only 4% of Amsterdam residents answer the survey and we predict the missing 96%? How do you expect the models to perform?

```{r}
set.seed(42)
train.idx <- sample.int(n=nrow(data), size=round(0.04*nrow(data)), replace=F)
train <-  (1:nrow(data) %in% train.idx)
test  <- !(1:nrow(data) %in% train.idx)

# How many rows?
print(paste(sum(train), sum(test), sep=" / "))
```

With this data set, train the models and calculate the AUC in the test set.

```{r class.source = 'fold-hide', results='hide'}

```

